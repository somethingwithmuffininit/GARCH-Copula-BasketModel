{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install yfinance numpy statsmodels pandas matplotlib arch pycop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# List of ticker symbols for each index\n",
    "tickers = [\"^GDAXI\", \"^AEX\", \"^N225\", \"^GSPC\", \"^GSPTSE\"]  # DAX, AEX, Nikkei 225, S&P 500, TSX\n",
    "index_datasets = {}\n",
    "\n",
    "\n",
    "# Loop over each ticker symbol to fetch the data\n",
    "for ticker in tickers:\n",
    "    # Retrieve the historical data starting from November 20, 1992\n",
    "    index = yf.Ticker(ticker)\n",
    "    \n",
    "    #Save to dictionary\n",
    "    index_datasets[ticker] = index.history(start=\"1992-11-20\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DataFrame to store merged data\n",
    "merged_data = pd.DataFrame()\n",
    "k=0\n",
    "# Loop over each ticker symbol to fetch the data\n",
    "for ticker in tickers:\n",
    "    \n",
    "    index_data = index_datasets[ticker]\n",
    "\n",
    "    # Extract only the Date and Close columns, reset the index\n",
    "    close_data = (index_data[['Close']]).reset_index()\n",
    "    \n",
    "\n",
    "\n",
    "    # Convert the Date column to Datetime\n",
    "    close_data['Date'] = pd.to_datetime((close_data['Date'].dt.normalize()).dt.tz_localize(None))\n",
    "\n",
    "    close_data.rename(columns={'Close': ticker},inplace=True)\n",
    "\n",
    "\n",
    "    \n",
    "    # Merge with the previously merged data\n",
    "    if merged_data.empty:\n",
    "        merged_data = close_data\n",
    "    else:\n",
    "        merged_data = pd.merge_asof(merged_data, close_data, on='Date', direction='nearest')\n",
    "\n",
    "\n",
    "\n",
    "for ticker in tickers:\n",
    "    #Forward fill missing data\n",
    "    merged_data[ticker] = merged_data[ticker].ffill()\n",
    "\n",
    "#Set Date as index\n",
    "merged_data.set_index('Date', inplace=True)\n",
    "#Resample at end of each quarter\n",
    "merged_data = merged_data.resample(\"QE\").last()\n",
    "\n",
    "#Compute Log Returns\n",
    "merged_data[:] = np.log(1+merged_data.pct_change())\n",
    "#merged_data[:] = merged_data.pct_change()\n",
    "\n",
    "merged_data.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^GDAXI Nu: 3.1515891575897332\n",
      "^AEX Nu: 3.2194036816496427\n",
      "^N225 Nu: 286.79481857586893\n",
      "^GSPC Nu: 3.4672641001969513\n",
      "^GSPTSE Nu: 3.149877563388971\n"
     ]
    }
   ],
   "source": [
    "from arch import arch_model\n",
    "from scipy.stats import t\n",
    "\n",
    "#Fit Student's t-GARCH\n",
    "def fit_garch(ts):\n",
    "    model = arch_model(ts, vol=\"Garch\", p=1, q=1, dist=\"t\")\n",
    "    fitted = model.fit(disp=\"off\")\n",
    "    residuals = ts - fitted.conditional_volatility  # Approximation for simplicity\n",
    "    nu = fitted.params[\"nu\"]\n",
    "    return residuals, nu\n",
    "\n",
    "\n",
    "nu_dict = {}\n",
    "residuals_dict = {}\n",
    "\n",
    "#We scale our returns by a factor of 100 for fitting as per the libraries recommendations\n",
    "for i in tickers:\n",
    "    residuals_dict[i], nu_dict[i] = fit_garch(merged_data[i]*100)\n",
    "    print(i+\" Nu: \"+str(nu_dict[i]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the large tails of returns demonstrated by each distibution, except the N225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "spearman_corr = spearmanr(list(residuals_dict.values()),axis=1)\n",
    "\n",
    "# Calculate the standard deviations of the residuals\n",
    "std_devs = []\n",
    "for i in tickers:\n",
    "    std_devs.append(np.std(residuals_dict[i]) )\n",
    "\n",
    "# Calculate the variance-covariance matrix\n",
    "cov_matrix = np.float64(spearman_corr) * np.outer(std_devs, std_devs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cov must be 2 dimensional and square",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[204], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#Number Monte Carlo Simulations\u001b[39;00m\n\u001b[0;32m      5\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m----> 7\u001b[0m simulated_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultivariate_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtickers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcov_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:4233\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.multivariate_normal\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cov must be 2 dimensional and square"
     ]
    }
   ],
   "source": [
    "\n",
    "#Seed\n",
    "np.random.seed(42) \n",
    "\n",
    "#Number Monte Carlo Simulations\n",
    "n = 1000\n",
    "\n",
    "simulated_data = np.random.multivariate_normal(np.zeros(len(tickers)),cov_matrix,n)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Transform the data to uniform margins using ECDF (Empirical CDF)\n",
    "uniform_data = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Use empirical CDF for each column (ticker) in the merged data\n",
    "for ticker in tickers:\n",
    "    uniform_data[ticker] = merged_data_filled[ticker].rank() / len(merged_data_filled[ticker])\n",
    "\n",
    "# Step 2: Fit a copula (using Gaussian copula as an example)\n",
    "copula = StudentTCopula()\n",
    "\n",
    "# Step 3: Fit the copula to the uniform-transformed data\n",
    "copula.fit(uniform_data)\n",
    "\n",
    "# Step 4: Simulate synthetic data from the fitted copula (optional)\n",
    "simulated_data = copula.sample(len(merged_data_filled))\n",
    "\n",
    "# Convert the uniform data back to the original scale (inverse of the transformation)\n",
    "simulated_data_original_scale = pd.DataFrame()\n",
    "\n",
    "for idx, ticker in enumerate(tickers):\n",
    "    # Inverse transform (since we used ECDF, we can approximate the inverse by using the quantiles)\n",
    "    simulated_data_original_scale[ticker] = np.percentile(merged_data_filled[ticker], simulated_data[:, idx] * 100)\n",
    "\n",
    "# View the simulated data\n",
    "print(simulated_data_original_scale.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycop\n",
      "  Downloading pycop-0.0.13-py3-none-any.whl.metadata (11 kB)\n",
      "Downloading pycop-0.0.13-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: pycop\n",
      "Successfully installed pycop-0.0.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pycop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
